{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb2d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (3.10.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install pandas  \n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25461505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed reviews:\n",
      "Shape: (1000, 1)\n",
      "Columns: ['Review']\n",
      "Total vocabulary size: 169\n",
      "First 20 words: ['acceptable', 'acre', 'actually', 'ad', 'amazing', 'ann', 'annoy', 'annoying', 'average', 'aweesome', 'awesoe', 'awesome', 'awesomee', 'awesomme', 'awesoome', 'awessome', 'awestme', 'awful', 'awmosme', 'awseome']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the processed reviews\n",
    "df = pd.read_csv('processed_reviews.csv')\n",
    "print(\"Loaded processed reviews:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()\n",
    "# First create vocabulary from your data\n",
    "odf = df.copy()\n",
    "reviews_text = odf['Review'].astype(str)\n",
    "\n",
    "# Build vocabulary from all reviews\n",
    "all_words = []\n",
    "for review in reviews_text:\n",
    "    words = review.split()\n",
    "    all_words.extend(words)\n",
    "\n",
    "vocabulary = sorted(list(set(all_words)))\n",
    "print(f\"Total vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"First 20 words: {vocabulary[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34585df4",
   "metadata": {},
   "source": [
    "##### One hot encoding\n",
    "It’s used to convert categorical data, such as words or the characters into a binary vector representation. Each unique category (word or character) is represented by a binary vector where only one element is “hot” (set to 1), while all others are “cold” (set to 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767b3947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying to your review data:\n",
      "Using vocabulary: ['acceptable', 'acre', 'actually', 'ad', 'amazing', 'ann', 'annoy', 'annoying', 'average', 'aweesome', 'awesoe', 'awesome', 'awesomee', 'awesomme', 'awesoome', 'awessome', 'awestme', 'awful', 'awmosme', 'awseome']\n",
      "\n",
      "Review 1: excellent experience service org...\n",
      "One-hot vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Words found: 0\n",
      "\n",
      "Review 2: horrible experience flight like fuck...\n",
      "One-hot vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Words found: 0\n",
      "\n",
      "Review 3: amazing experience service...\n",
      "One-hot vector: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Words found: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now apply to your actual data\n",
    "print(\"Applying to your review data:\")\n",
    "\n",
    "# Use first 3 reviews and first 20 words from vocabulary\n",
    "sample_reviews = reviews_text.head(3)\n",
    "small_vocab = vocabulary[:20]\n",
    "\n",
    "print(f\"Using vocabulary: {small_vocab}\")\n",
    "print()\n",
    "\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    # Create one-hot vector\n",
    "    one_hot_vector = [0] * len(small_vocab)\n",
    "    \n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        if word in small_vocab:\n",
    "            word_index = small_vocab.index(word)\n",
    "            one_hot_vector[word_index] = 1\n",
    "    \n",
    "    print(f\"Review {i+1}: {review[:50]}...\")\n",
    "    print(f\"One-hot vector: {one_hot_vector}\")\n",
    "    print(f\"Words found: {sum(one_hot_vector)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655197a9",
   "metadata": {},
   "source": [
    "I dont think One Hot encoding would be a good choice for the sentiment analysis project..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4206ac",
   "metadata": {},
   "source": [
    "## Index Based Encoding\n",
    "Index-based encoding is a simple text encoding technique where each word in the vocabulary is assigned a unique integer index. Instead of using binary vectors like one-hot encoding, we represent text as sequences of these integer indices.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Create a vocabulary dictionary mapping words to unique numbers\n",
    "Convert each text into a sequence of numbers\n",
    "Each word becomes its corresponding index number\n",
    "Example:\n",
    "\n",
    "Advantages over One-Hot:\n",
    "\n",
    "Memory efficient - stores integers instead of long binary vectors\n",
    "Faster processing - smaller data size\n",
    "Variable length - can handle texts of different lengths naturally\n",
    "Disadvantages:\n",
    "\n",
    "No semantic meaning - index 5 isn't \"more\" than index 2\n",
    "Order dependency - models might think higher indices are more important\n",
    "Padding needed - for fixed-length inputs in some models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93bf341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Based Encoding\n",
      "Sample word mappings:\n",
      "'acceptable' -> 0\n",
      "'acre' -> 1\n",
      "'actually' -> 2\n",
      "'ad' -> 3\n",
      "'amazing' -> 4\n",
      "'ann' -> 5\n",
      "'annoy' -> 6\n",
      "'annoying' -> 7\n",
      "'average' -> 8\n",
      "'aweesome' -> 9\n",
      "\n",
      "Review 1: excellent experience service o...\n",
      "Indices: [40, 41, 136, 106]...\n",
      "Total words: 4\n",
      "\n",
      "Review 2: horrible experience flight lik...\n",
      "Indices: [64, 41, 50, 74, 54]...\n",
      "Total words: 5\n",
      "\n",
      "Review 3: amazing experience service...\n",
      "Indices: [4, 41, 136]...\n",
      "Total words: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Index Based Encoding\")\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(vocabulary):\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(\"Sample word mappings:\")\n",
    "for i in range(10):\n",
    "    word = vocabulary[i]\n",
    "    print(f\"'{word}' -> {i}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Encode fpr the first 3 reviews\n",
    "for i in range(3):\n",
    "    review = reviews_text.iloc[i]\n",
    "    words = review.split()\n",
    "    \n",
    "    indices = []\n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            indices.append(word_to_index[word])\n",
    "    \n",
    "    print(f\"Review {i+1}: {review[:30]}...\")\n",
    "    print(f\"Indices: {indices[:5]}...\")\n",
    "    print(f\"Total words: {len(indices)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc465e",
   "metadata": {},
   "source": [
    "### BAG OF WORDS (BOW)\n",
    "BAG OF WORDS (BOW)\n",
    "Bag of Words is a text encoding technique that represents text as a collection (or \"bag\") of words, ignoring grammar and word order but keeping track of word frequency. It's called a \"bag\" because it treats text like throwing words into a bag - you know what words are there and how many times each appears, but not their order.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Create a vocabulary from all documents\n",
    "For each document, count how many times each word appears\n",
    "Represent each document as a vector of word counts\n",
    "\n",
    "Example:\n",
    "Review 1: \"good product good quality\"\n",
    "Review 2: \"bad product terrible quality\"\n",
    "\n",
    "Vocabulary: [\"bad\", \"good\", \"product\", \"quality\", \"terrible\"]\n",
    "\n",
    "Review 1 BOW: [0, 2, 1, 1, 0]  # good appears 2 times\n",
    "Review 2 BOW: [1, 0, 1, 1, 1]  # each word appears 1 time\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Captures word frequency - \"very very good\" vs \"good\" are different\n",
    "Simple to understand and implement\n",
    "Works well for many text classification tasks\n",
    "Better than one-hot for sentiment analysis\n",
    "Disadvantages:\n",
    "\n",
    "Ignores word order - \"good bad\" vs \"bad good\" look the same\n",
    "No semantic meaning - doesn't know \"good\" and \"great\" are similar\n",
    "Sparse vectors - most values are 0 for large vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a17c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Encoding:\n",
      "Vocabulary: ['acceptable', 'acre', 'actually', 'ad', 'amazing', 'ann', 'annoy', 'annoying', 'average', 'aweesome', 'awesoe', 'awesome', 'awesomee', 'awesomme', 'awesoome', 'awessome', 'awestme', 'awful', 'awmosme', 'awseome']\n",
      "\n",
      "Review 1: excellent experience service org...\n",
      "BOW vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total word count: 0\n",
      "\n",
      "Review 2: horrible experience flight like fuck...\n",
      "BOW vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total word count: 0\n",
      "\n",
      "Review 3: amazing experience service...\n",
      "BOW vector: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total word count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words Encoding:\")\n",
    "\n",
    "sample_reviews = reviews_text.head(3)\n",
    "small_vocab = vocabulary[:20]\n",
    "\n",
    "print(f\"Vocabulary: {small_vocab}\")\n",
    "print()\n",
    "\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    # Create count vector\n",
    "    bow_vector = [0] * len(small_vocab)\n",
    "    \n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        if word in small_vocab:\n",
    "            word_index = small_vocab.index(word)\n",
    "            bow_vector[word_index] += 1  # Count frequency\n",
    "    \n",
    "    print(f\"Review {i+1}: {review[:40]}...\")\n",
    "    print(f\"BOW vector: {bow_vector}\")\n",
    "    print(f\"Total word count: {sum(bow_vector)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d6d93",
   "metadata": {},
   "source": [
    "### TF-IDF Encoding (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF is kinda a sophisticated text encoding tchnq which considers both the how a word appears frequently in the document (TF wala part) and how rare or common the word is across all the documents rey k hai(IDF wala part). so what it does is ki it helps identify the words that r imp to a specific document\n",
    "\n",
    "How it works:\n",
    "TF (Term Frequency): How often a word appears in a document\n",
    "IDF (Inverse Document Frequency): How rare/common a word is across all documents\n",
    "TF-IDF = TF × IDF: Words that appear frequently in one document but rarely in others get higher scores\n",
    "\n",
    "Example:\n",
    "Document 1: \"good good product\"\n",
    "Document 2: \"bad product\"\n",
    "Word \"good\": appears 2 times in doc1, 0 times in doc2 → high TF-IDF in doc1\n",
    "Word \"product\": appears in both docs → lower IDF score\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Identifies important words - filters out common words like \"the\", \"and\"\n",
    "Better for sentiment analysis - emphasizes distinctive words\n",
    "Reduces noise - common words get lower weights\n",
    "Disadvantages:\n",
    "\n",
    "More complex than simple counting\n",
    "Still ignores word order\n",
    "Can be computationally expensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e7ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF using sklearn:\n",
      "TF-IDF matrix shape: (1000, 100)\n",
      "Feature names (first 10): ['acceptable' 'actually' 'amazing' 'annoy' 'annoying' 'average' 'awesome'\n",
      " 'awessome' 'awestme' 'awful']\n",
      "Review 1 TF-IDF vector (first 10 features):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Non-zero elements: 4\n",
      "\n",
      "Review 2 TF-IDF vector (first 10 features):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Non-zero elements: 5\n",
      "\n",
      "Review 3 TF-IDF vector (first 10 features):\n",
      "[0.         0.         0.70482456 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Non-zero elements: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF using sklearn:\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Use top 100 words\n",
    "    stop_words='english'  # Remove common English words\n",
    ")\n",
    "\n",
    "# Fit and transform the reviews\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews_text)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Feature names (first 10): {tfidf_vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Show sample TF-IDF vectors\n",
    "sample_tfidf = tfidf_matrix[:3].toarray()\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Review {i+1} TF-IDF vector (first 10 features):\")\n",
    "    print(f\"{sample_tfidf[i][:10]}\")\n",
    "    print(f\"Non-zero elements: {np.count_nonzero(sample_tfidf[i])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60627d4d",
   "metadata": {},
   "source": [
    "## Word2Vector Encoding:\n",
    "Word2Vector (Word2Vec) is a neural network-based technique that converts words into dense vector representations where words with similar meanings have similar vector representations. Unlike previous methods, Word2Vec captures semantic relationships between words.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Uses neural networks to learn word representations from large text corpora\n",
    "Maps each word to a dense vector (typically 100-300 dimensions)\n",
    "Words with similar contexts get similar vectors\n",
    "Captures semantic relationships like \"king - man + woman = queen\"\n",
    "Two main approaches:\n",
    "\n",
    "CBOW (Continuous Bag of Words): Predicts target word from context words\n",
    "Skip-gram: Predicts context words from target word\n",
    "Advantages:\n",
    "\n",
    "Captures semantic meaning - \"good\" and \"great\" have similar vectors\n",
    "Dense vectors - no sparse data issues\n",
    "Semantic relationships - can do word arithmetic\n",
    "Transfer learning - can use pre-trained models\n",
    "Disadvantages:\n",
    "\n",
    "Requires large datasets for training\n",
    "Fixed vocabulary - can't handle new words\n",
    "No word order - still ignores sentence structure\n",
    "Computationally expensive to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e0b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gensim 4.3.2\n",
      "Uninstalling gensim-4.3.2:\n",
      "  Successfully uninstalled gensim-4.3.2\n",
      "Found existing installation: numpy 1.24.3\n",
      "Uninstalling numpy-1.24.3:\n",
      "  Successfully uninstalled numpy-1.24.3\n",
      "Found existing installation: scipy 1.13.1\n",
      "Uninstalling scipy-1.13.1:\n",
      "  Successfully uninstalled scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'D:\\Aayush_Acharya\\GENAI-Bootcamp\\GENAI\\Projects\\Proj-2\\.venv\\Lib\\site-packages\\~-mpy'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'D:\\Aayush_Acharya\\GENAI-Bootcamp\\GENAI\\Projects\\Proj-2\\.venv\\Lib\\site-packages\\~-ipy.libs'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'D:\\Aayush_Acharya\\GENAI-Bootcamp\\GENAI\\Projects\\Proj-2\\.venv\\Lib\\site-packages\\~-ipy'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp310-cp310-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting scipy==1.10.1\n",
      "  Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl.metadata (58 kB)\n",
      "Collecting gensim==4.3.2\n",
      "  Using cached gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from gensim==4.3.2) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in d:\\aayush_acharya\\genai-bootcamp\\genai\\projects\\proj-2\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.3)\n",
      "Using cached numpy-1.24.3-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "   ---------------------------------------- 0.0/42.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/42.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/42.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/42.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/42.5 MB 1.9 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.6/42.5 MB 2.1 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.8/42.5 MB 2.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 3.1/42.5 MB 2.9 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 3.1/42.5 MB 2.9 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.7/42.5 MB 3.1 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 4.7/42.5 MB 3.1 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 5.8/42.5 MB 3.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 7.1/42.5 MB 3.3 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 9.2/42.5 MB 3.8 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 10.5/42.5 MB 4.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.5/42.5 MB 4.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.5/42.5 MB 4.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.5/42.5 MB 4.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.5/42.5 MB 4.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 10.7/42.5 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 11.5/42.5 MB 2.9 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 12.3/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 12.3/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 13.6/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.9/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.7/42.5 MB 2.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 14.9/42.5 MB 2.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 15.2/42.5 MB 2.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 15.5/42.5 MB 2.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 16.0/42.5 MB 2.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 16.5/42.5 MB 2.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 16.8/42.5 MB 2.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 17.3/42.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 17.6/42.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 18.1/42.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 18.4/42.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 18.6/42.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.1/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.4/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.7/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 19.9/42.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 21.0/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 21.2/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 21.8/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 22.0/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 22.5/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 22.8/42.5 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 23.3/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 23.6/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 23.9/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 24.4/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 24.6/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 25.2/42.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 25.7/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 26.0/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 26.2/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 26.7/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 27.0/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 27.5/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 27.8/42.5 MB 2.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 28.0/42.5 MB 2.1 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 28.0/42.5 MB 2.1 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 28.8/42.5 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 29.1/42.5 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 29.6/42.5 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 29.9/42.5 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 30.1/42.5 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 30.4/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 30.7/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 31.2/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 31.5/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 32.0/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 32.2/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 32.5/42.5 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 32.8/42.5 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 33.0/42.5 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 33.8/42.5 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 34.1/42.5 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 34.6/42.5 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 34.6/42.5 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 35.4/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.7/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.2/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.7/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.2/42.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.7/42.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.3/42.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 39.1/42.5 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.6/42.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.4/42.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 41.2/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.4/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.7/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.7/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.7/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.5/42.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.5/42.5 MB 2.0 MB/s  0:00:22\n",
      "Using cached gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   ---------------------------------------- 3/3 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.3.2 numpy-1.24.3 scipy-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Uninstall everything and reinstall compatible versions\n",
    "%pip uninstall -y gensim numpy scipy\n",
    "%pip install numpy==1.24.3 scipy==1.10.1 gensim==4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ab357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building cooccurrence matrix for 50 words...\n",
      "Cooccurrence matrix shape: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build cooccurrence matrix\n",
    "def build_cooccurrence_matrix(sentences, vocab, window_size=2):\n",
    "    vocab_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word in vocab_to_idx:\n",
    "                target_idx = vocab_to_idx[word]\n",
    "                # Look at surrounding words\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(words), i + window_size + 1)\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    if i != j and words[j] in vocab_to_idx:\n",
    "                        context_idx = vocab_to_idx[words[j]]\n",
    "                        matrix[target_idx][context_idx] += 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "small_vocab = vocabulary[:50]  # first 50 words\n",
    "sample_sentences = reviews_text.head(100).tolist()  # first 100 reviews\n",
    "\n",
    "print(f\"Building cooccurrence matrix for {len(small_vocab)} words...\")\n",
    "cooc_matrix = build_cooccurrence_matrix(sample_sentences, small_vocab)\n",
    "\n",
    "print(f\"Cooccurrence matrix shape: {cooc_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cafa82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word vectors using SVD:\n",
      "Word vectors shape: (50, 50)\n",
      "\n",
      "Sample word vectors:\n",
      "good: not in vocab\n",
      "bad: [1.87918622 1.33824214 0.32252031]...\n",
      "product: not in vocab\n",
      "quality: not in vocab\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"Creating word vectors using SVD:\")\n",
    "\n",
    "# Applying SVD to reduce dimensions\n",
    "n_components = 50  # 50-dimensional vectors\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "word_vectors = svd.fit_transform(cooc_matrix)\n",
    "\n",
    "print(f\"Word vectors shape: {word_vectors.shape}\")\n",
    "\n",
    "# Create word to vector mapping\n",
    "word_to_vector = {}\n",
    "for i, word in enumerate(small_vocab):\n",
    "    word_to_vector[word] = word_vectors[i]\n",
    "\n",
    "# Show some word vectors\n",
    "print(\"\\nSample word vectors:\")\n",
    "for word in ['good', 'bad', 'product', 'quality']:\n",
    "    if word in word_to_vector:\n",
    "        vector = word_to_vector[word]\n",
    "        print(f\"{word}: {vector[:3]}...\")  # first 3 dimensions\n",
    "    else:\n",
    "        print(f\"{word}: not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec81821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word similarities:\n"
     ]
    }
   ],
   "source": [
    "# Calculate word similarities\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "print(\"Word similarities:\")\n",
    "if 'awesome' in word_to_vector:\n",
    "    awesome_vec = word_to_vector['awesome']\n",
    "    similarities = []\n",
    "    \n",
    "    for word in small_vocab:\n",
    "        if word != 'awesome' and word in word_to_vector:\n",
    "            sim = cosine_similarity(awesome_vec, word_to_vector[word])\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Words most similar to 'awesome':\")\n",
    "    for word, sim in similarities[:5]:\n",
    "        print(f\"{word}: {sim:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
